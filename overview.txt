Summary: NYC Restaurant Inspection Grade Prediction Project
1. Dataset Overview
Dataset: NYC Restaurant Inspection Data (NYC_Inspection_data.csv)
Initial size: 103,426 rows × 26 columns
Final size: 83,603 rows × 26 columns (after cleaning)
Time period: 2012–2023
Unique restaurants: 21,165
Target variable: GRADE (A, B, or C)
Class A: 68,421 (81.9%)
Class B: 9,972 (11.9%)
Class C: 5,210 (6.2%)

2. Exploratory Data Analysis (EDA)
Findings:
Severe class imbalance: ~82% Grade A, ~12% Grade B, ~6% Grade C
Score distribution: Scores range from 0 (best) to higher values; negative correlation with grades
Geographic patterns: Grades vary by borough
Manhattan: 25,955 Grade A inspections
Brooklyn: 17,929 Grade A inspections
Queens: 15,828 Grade A inspections
Temporal trends: Average inspection scores increased over time (7.0 in 2012 → 13.9 in 2023)
Statistical tests:
Cramér's V (categorical vs GRADE):
INSPECTION TYPE: 0.3738 (strongest)
CUISINE DESCRIPTION: 0.1514
BORO: 0.0302
Kruskal–Wallis (numeric features):
SCORE: p < 0.001
Longitude: p < 0.001
Latitude: p = 0.053

3. Data Pre-processing
Cleaning steps:
Removed duplicates: 0 duplicates found
Converted date columns to datetime: INSPECTION DATE, GRADE DATE, RECORD DATE
Handled missing data:
Dropped 50 rows with missing SCORE
Dropped 19,773 rows with non-A/B/C grades (Closed, N, P, Z)
Data type conversions:
ZIPCODE → string
Categorical columns set to category type
Feature engineering:
Extracted INSPECTION_YEAR and INSPECTION_MONTH from INSPECTION DATE
Train-test split:
Method: Grouped Shuffle Split (70/30) by restaurant ID (CAMIS)
Rationale: Prevents data leakage (same restaurant not in both sets)
Train: 58,336 samples
Test: 25,267 samples

4. Features Used for Modeling
6 pre-inspection features (known before inspection):
Geographic:
Latitude (numeric)
Longitude (numeric)
BORO (categorical: Bronx, Brooklyn, Manhattan, Queens, Staten Island)
Restaurant characteristics:
CUISINE DESCRIPTION (categorical)
Temporal:
INSPECTION_YEAR (numeric: 2012–2023)
INSPECTION_MONTH (numeric: 1–12)
Features excluded (to avoid leakage):
SCORE (directly determines GRADE)
VIOLATION CODE/DESCRIPTION (determined during inspection)
INSPECTION TYPE (can leak outcome information)
Post-inspection features
Preprocessing:
Categorical encoding: OneHotEncoder for BORO and CUISINE DESCRIPTION
Numeric features: Pass-through for Latitude, Longitude, INSPECTION_YEAR, INSPECTION_MONTH

5. Handling Class Imbalance
Two approaches:
A. Class Weight Balancing
Applied to all models
Automatically adjusts weights inversely proportional to class frequency
B. SMOTE (Synthetic Minority Oversampling Technique)
Upsampled minority classes (B and C) to match majority class (A)
Before SMOTE: A=82.0%, B=11.8%, C=6.3%
After SMOTE: A=33.3%, B=33.3%, C=33.3%
Training set increased from 58,336 to 143,448 samples (+145.9%)

6. Models Used and Rationale
1. Logistic Regression (Baseline)
Type: Multinomial logistic regression
Why: Linear baseline, interpretable coefficients
Configuration:
Solver: LBFGS
Max iterations: 1500
Class weight: balanced
Expected: Lower performance due to non-linear patterns
2. Random Forest
Type: Ensemble of 300 decision trees
Why: Handles non-linear patterns, geographic partitions, feature interactions
Configuration:
n_estimators: 300
Class weight: balanced
Expected: Strong performance on geographic/categorical patterns
3. XGBoost
Type: Gradient boosting
Why: Effective for tabular data, handles imbalance
Configuration:
n_estimators: 300
max_depth: 4
learning_rate: 0.1
subsample: 0.8
colsample_bytree: 0.8
Sample weights for class imbalance
Expected: Good performance on imbalanced data
4. CatBoost
Type: Gradient boosting with categorical handling
Why: Handles categorical features natively, robust to overfitting
Configuration:
iterations: 300
depth: 4
learning_rate: 0.1
Class weights for imbalance
Expected: Competitive performance, especially with categorical features

7. Model Performance Results
Test Set Accuracy (Class Weight Approach):
Random Forest: 80.08%
XGBoost: 57.65%
CatBoost: ~9.57% (appears to have an issue; needs investigation)
Logistic Regression: 48.76%
Test Set Accuracy (SMOTE Approach):
Random Forest: 78.28%
XGBoost: 58.58%
CatBoost: 55.68%
Logistic Regression: 48.56%
Key observations:
Random Forest performs best: ~80% test accuracy with class weights, ~78% with SMOTE
Overfitting in Random Forest: Train accuracy 99.9%, test accuracy ~80%
SMOTE impact: Slight accuracy decrease for Random Forest, slight improvement for XGBoost and CatBoost
Class imbalance challenge: Models struggle with B and C classes; most predictions default to Grade A
Performance metrics (Class Weight, Test Set):
Random Forest: Accuracy 80.08%, Precision 0.71, Recall 0.80, F1-Score 0.74
XGBoost: Accuracy 57.65%, Precision 0.75, Recall 0.58, F1-Score 0.64
Logistic Regression: Accuracy 48.76%, Precision 0.48, Recall 0.48, F1-Score 0.48

8. Visualizations Generated
Class distribution (before/after SMOTE)
Model accuracy comparison (Class Weight vs SMOTE)
Per-class F1-scores for each model
Confusion matrices (all models, both approaches)
Feature importance (top 10 for each model)
Geographic distribution of grades
Model performance comparison (Accuracy, Precision, Recall, F1-Score)

9. Key Insights
Random Forest is the best model: ~80% test accuracy, captures geographic and categorical patterns
Class imbalance is a major challenge: Models favor Grade A; B and C are underrepresented
SMOTE trade-off: Slight accuracy decrease for Random Forest, but may improve recall for minority classes
Feature importance: Geographic features (Latitude, Longitude) and temporal features are most predictive
Limitations: Using only pre-inspection features limits predictive power; historical behavior data could help
10. Conclusion
The project predicts NYC restaurant inspection grades using pre-inspection features. Random Forest achieved ~80% accuracy, but class imbalance remains a challenge. The analysis compares four models and two imbalance-handling techniques, providing insights into factors that may influence inspection outcomes.